{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215f0450",
   "metadata": {},
   "source": [
    "## 1. Explain succinctly in your own words (but working with a ChatBot if needed)...<br>\n",
    "\n",
    "1. the difference between **Simple Linear Regression** and **Multiple Linear Regression**; and the benefit the latter provides over the former\n",
    "\n",
    "\n",
    "2. the difference between using a **continuous variable** and an **indicator variable** in **Simple Linear Regression**; and these two **linear forms**\n",
    "\n",
    "\n",
    "3. the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single **indicator variable** is introduced alongside a **continuous variable** to create a **Multiple Linear Regression**; and these two **linear forms** (i.e., the **Simple Linear Regression** versus the **Multiple Linear Regression**)\n",
    "\n",
    "\n",
    "4. the effect of adding an **interaction** between a **continuous** and an **indicator variable** in **Multiple Linear Regression** models; and this **linear form**\n",
    "\n",
    "\n",
    "5. the behavior of a **Multiple Linear Regression** model (i.e., the expected nature of the data it models) based only on **indicator variables** derived from a **non-binary categorical variable**; this **linear form**; and the necessarily resulting **binary variable encodings** it utilizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16226077",
   "metadata": {},
   "source": [
    "1. **Simple Linear Regression vs. Multiple Linear Regression:**\n",
    "   - **Simple Linear Regression** involves a single predictor variable to model the relationship with the outcome variable.\n",
    "   - **Multiple Linear Regression** involves two or more predictor variables. The benefit of multiple regression is that it accounts for the combined influence of various predictors, often improving accuracy by reducing omitted variable bias and capturing more complex relationships in the data.\n",
    "\n",
    "2. **Continuous vs. Indicator Variable in Simple Linear Regression:**\n",
    "   - A **continuous variable** can take a wide range of values (e.g., age, income) and provides a slope that represents the change in the outcome for a one-unit change in the predictor.\n",
    "   - An **indicator variable** (or binary variable) typically represents categorical data (e.g., gender) and takes on values of 0 or 1. It shifts the intercept depending on the category, leading to a segmented linear form where the mean outcome differs by group but not the slope.\n",
    "\n",
    "3. **Adding an Indicator Variable to a Continuous Variable in Multiple Linear Regression:**\n",
    "   - Introducing an indicator variable alongside a continuous variable in **Multiple Linear Regression** allows the model to estimate separate intercepts for each category represented by the indicator variable while maintaining a common slope for the continuous predictor. This creates a parallel line model where each category shifts vertically but maintains a similar trend.\n",
    "\n",
    "4. **Interaction between Continuous and Indicator Variable in Multiple Linear Regression:**\n",
    "   - Including an **interaction term** between a continuous variable and an indicator variable allows the slope of the continuous variable to differ across categories. This results in non-parallel lines, as each category can now have a unique slope and intercept, offering a more tailored model to represent varying rates of change across groups.\n",
    "\n",
    "5. **Multiple Linear Regression with Only Indicator Variables from a Categorical Variable:**\n",
    "   - When a **non-binary categorical variable** is represented in the model, it’s typically encoded as multiple **binary variables** (dummy variables) for each category. This structure forms a set of parallel lines (one per category) and each coefficient (except the reference category) represents a shift in the intercept, modeling differences in group means across categories without a continuous slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86309d90",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "1. **Simple vs. Multiple Linear Regression:** Simple regression has one predictor; multiple regression has multiple predictors, which helps capture more complex relationships and improves accuracy.\n",
    "\n",
    "2. **Continuous vs. Indicator Variable in Simple Regression:** A continuous variable has a slope showing change per unit increase, while an indicator variable shifts the intercept based on category without affecting the slope.\n",
    "\n",
    "3. **Adding an Indicator to a Continuous Variable in Multiple Regression:** This adds separate intercepts for each category but keeps a common slope, creating parallel lines for each category.\n",
    "\n",
    "4. **Interaction between Continuous and Indicator in Multiple Regression:** An interaction allows both slope and intercept to differ by category, creating non-parallel lines and capturing unique trends for each group.\n",
    "\n",
    "5. **Multiple Regression with Only Indicator Variables:** A categorical variable is split into binary indicators (dummy variables) for each category, creating parallel lines that represent shifts in the intercept for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71c78a",
   "metadata": {},
   "source": [
    "### 2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered<br>\n",
    "\n",
    "> Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.    \n",
    "\n",
    "1. Explain how to use these two formulas to make **predictions** of the **outcome**, and give a high level explaination in general terms of the difference between **predictions** from the models with and without the **interaction** \n",
    "\n",
    "2. Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda5b03",
   "metadata": {},
   "source": [
    "### Outcome and predictor variables\n",
    "- **Outcome variable**: the company wants the predicted outcome to be sales, which depends on the investment in advertising.\n",
    "- **Predictor variable**:\n",
    "  - Television advertising expenditures\n",
    "  - Online advertising expenditures\n",
    "\n",
    "### Whether interaction effects need to be considered\n",
    "The effectiveness of TV advertising may change depending on the level of spending on online advertising and vice versa. This suggests that there may be an interaction effect between the two forms of advertising.\n",
    "\n",
    "### Models with and without interaction\n",
    "- **Without interaction model**: This model assumes that TV advertising and online advertising each affect sales independently. The effects of each type of advertising do not affect each other.\n",
    "- **Model with Interaction**: This model takes into account the synergistic effects of the two types of advertising. For example, when TV ad spending is high, the effect of online ads may be stronger (or weaker).\n",
    "\n",
    "### Predictive differences between the two models\n",
    "- **No Interaction Model**: It only considers the effect of individual ad spending on sales and ignores the effect of synergy between ads. Its predictions are simpler, but may not be accurate enough.\n",
    "- **With interaction model**: Considering both the independent effect of each advertisement and their synergistic effect, the prediction result is closer to the actual situation, especially if the two advertisements do affect each other.\n",
    "\n",
    "### If ad budgets are categorized as “high/low\n",
    "If the advertising budget is categorized as “high” or “low” (instead of a specific amount):\n",
    "- **Without interaction model**: In the forecast, only the separate effects of “high” or “low” TV and online advertising on sales are considered.\n",
    "- **With Interaction Model**: In the forecast, whether “high” TV ads and “high” online ads will have an additional effect is considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d874f40",
   "metadata": {},
   "source": [
    "### Summary\n",
    "If there is a real interaction between the two ads, the model with interaction will predict sales more accurately. However, if the effects of the ads are completely independent, a model without interaction is sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6436a",
   "metadata": {},
   "source": [
    "### 3. Use *smf* to fit *multiple linear regression* models to the course project dataset from the canadian social connection survey<br>\n",
    "\n",
    "> **EDIT: No, you probably actually care about CATEGORICAL or BINARY outcomes rather than CONTINUOUS outcomes... so you'll probably not actually want to do _multiple linear regression_ and instead do _logistic regression_ or _multi-class classification_. Okay, I'll INSTEAD guide you through doing _logistic regression_.**\n",
    "\n",
    "1. ~~for an **additive** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables** and a **CONTINUOUS OUTCOME varaible**~~ \n",
    "    1. This would have been easy to do following the instructions [here](https://www.statsmodels.org/dev/example_formulas.html). A good alternative analagous presentation for logistic regression I just found seems to be this one from a guy named [Andrew](https://www.andrewvillazon.com/logistic-regression-python-statsmodels/). He walks you through the `logit` alternative to `OLS` given [here](https://www.statsmodels.org/dev/api.html#discrete-and-count-models).\n",
    "    2. Logistic is for a **binary outcome** so go see this [piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1) describing how you can turn any **non-binary categorical variable** into a **binary variable**. \n",
    "    3. Then instead do this problem like this: **catogorical outcome** turned into a **binary outcome** for **logistic regression** and then use any **additive** combination of a couple of **continuous**, **binary**, and/or **categorical variables** as **predictor variables**. \n",
    "\n",
    "\n",
    "```python\n",
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()\n",
    "```\n",
    "\n",
    "\n",
    "2. ~~for a **synertistic interaction** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables**~~\n",
    "    1. But go ahead and AGAIN do this for **logistic regression** like above.\n",
    "    2. Things are going to be A LOT simpler if you restrict yourself to **continuous** and/or **binary predictor variables**.  But of course you could *use the same trick again* to treat any **categorical variable** as just a **binary variable** (in the manner of [that piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1).\n",
    "    \n",
    "\n",
    "3. and **interpretively explain** your **linear forms** and how to use them to make **predictions**\n",
    "    1. Look, intereting **logistic regression** *IS NOT* as simple as interpreting **multivariate linear regression**. This is because it requires you to understand so-called **log odds** and that's a bit tricky. \n",
    "    2. So, INSTEAD, **just intepret you logistic regression models** *AS IF* they were **multivariate linear regression model predictions**, okay?\n",
    "\n",
    "\n",
    "4. and interpret the statistical evidence associated with the **predictor variables** for each of your model specifications \n",
    "    1. **Yeah, you're going to be able to do this based on the `.fit().summary()` table _just like with multiple linear regression_**... now you might be starting to see how AWESOME all of this stuff we're doing is going to be able to get...\n",
    "\n",
    "\n",
    "5. and finally use `plotly` to visualize the data with corresponding \"best fit lines\" for a model with **continuous** plus **binary indicator** specification under both (a) **additive** and (b) **synergistic** specifications of the **linear form** (on separate figures), commenting on the apparent necessity (or lack thereof) of the **interaction** term for the data in question\n",
    "    1. Aw, shit, you DEF not going to be able to do this if you're doing **logistic regression** because of that **log odds** thing I mentioned... hmm...\n",
    "    2. OKAY! Just *pretend* it's **multivariate linear regression** (even if you're doing **logistic regression**) and *pretend* your **fitted coefficients** belong to a **continuous** and a **binary predictor variable**; then, draw the lines as requested, and simulate **random noise** for the values of your **predictor data** and plot your lines along with that data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48a7fe",
   "metadata": {},
   "source": [
    "### Step 1: Define an Additive Logistic Regression Model\n",
    "Since we're using logistic regression for a **binary outcome**, start by converting a categorical outcome to a binary variable. Let’s take some combination of **continuous**, **binary**, and **categorical** variables as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95563f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load your Canadian Social Connection Survey dataset\n",
    "# Assuming `df` is your loaded dataset\n",
    "# Convert a categorical outcome to binary\n",
    "df['high_social_support'] = (df['social_support_score'] > threshold).astype(int)  # Adjust threshold as needed\n",
    "\n",
    "# Example predictors (e.g., age as continuous, gender as binary, and marital status as categorical)\n",
    "# You could use 'age', 'is_male' (binary), and 'marital_status' (categorical)\n",
    "\n",
    "# Additive model formula for logistic regression\n",
    "additive_formula = 'high_social_support ~ age + is_male + C(marital_status)'\n",
    "logit_additive = smf.logit(additive_formula, data=df).fit()\n",
    "print(logit_additive.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bd9f0",
   "metadata": {},
   "source": [
    "### Step 2: Define an Interaction (Synergistic) Logistic Regression Model\n",
    "For an interaction model, introduce a term that captures the combined effect of two variables. Here, `*` between predictors specifies interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650574a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction model formula with an interaction between age and gender\n",
    "interaction_formula = 'high_social_support ~ age * is_male + C(marital_status)'\n",
    "logit_interaction = smf.logit(interaction_formula, data=df).fit()\n",
    "print(logit_interaction.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82dfd05",
   "metadata": {},
   "source": [
    "### Step 3: Interpret the Results\n",
    "In the `.summary()` table:\n",
    "- **Coefficients**: Describe the effect of each predictor on the log odds of the outcome. A positive coefficient means higher values of that predictor increase the likelihood of a high social support score.\n",
    "- **P-values**: Check if each predictor is statistically significant. If \\( p < 0.05 \\), the predictor significantly impacts the outcome.\n",
    "- **Odds Ratio Interpretation**: You can interpret each coefficient as if it were a linear predictor for simplicity (just remember that they represent log odds, not raw values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340afef",
   "metadata": {},
   "source": [
    "### Step 4: Visualize the Model\n",
    "Although logistic regression predicts log odds, you can approximate visualizations with lines. Use `plotly` to plot \"best fit lines\" for a **continuous variable (e.g., age)** plus a **binary indicator (e.g., is_male)**. For both models:\n",
    "1. Generate points for a range of values for `age`.\n",
    "2. Draw separate lines for `is_male` = 0 and `is_male` = 1 based on the logistic regression prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Generate age values and compute predicted probabilities for both binary values of is_male\n",
    "ages = np.linspace(df['age'].min(), df['age'].max(), 100)\n",
    "pred_data = pd.DataFrame({'age': np.tile(ages, 2), 'is_male': np.repeat([0, 1], 100)})\n",
    "\n",
    "# Add predictions from both models\n",
    "pred_data['additive_prediction'] = logit_additive.predict(pred_data)\n",
    "pred_data['interaction_prediction'] = logit_interaction.predict(pred_data)\n",
    "\n",
    "# Plot using plotly\n",
    "fig = px.line(pred_data, x='age', y='additive_prediction', color='is_male',\n",
    "              title='Additive Model - Predicted Probability by Age and Gender')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(pred_data, x='age', y='interaction_prediction', color='is_male',\n",
    "              title='Interaction Model - Predicted Probability by Age and Gender')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637d88a",
   "metadata": {},
   "source": [
    "### Step 5: Comment on the Interaction Term's Necessity\n",
    "Once you've generated the visualizations:\n",
    "- **Compare the lines**: If the lines from the interaction model show different trends or diverge significantly based on `is_male`, this suggests the interaction term is meaningful.\n",
    "- **Without Interaction**: If the lines overlap heavily or show similar patterns in both models, the interaction term might not be essential.\n",
    "\n",
    "This approach allows you to conduct logistic regression, interpret the log odds in a simplified way, and visually assess whether the interaction provides additional insights. Let me know if you’d like help with specific parts of the code or interpretation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9360d8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### 1. Define the Logistic Regression Model\n",
    "   - Convert a categorical outcome (e.g., social support level) into a binary variable.\n",
    "   - Choose a few predictor variables, including continuous (e.g., age), binary (e.g., gender), and categorical (e.g., marital status).\n",
    "\n",
    "### 2. Fit Additive and Interaction Models\n",
    "   - **Additive Model**: Fit a model with predictors entered independently.\n",
    "   - **Interaction Model**: Add an interaction term to capture the combined effect between two variables (e.g., age and gender).\n",
    "\n",
    "### 3. Interpret Results\n",
    "   - Use the `.summary()` table to interpret coefficients and p-values:\n",
    "      - **Coefficients**: Estimate the effect of each predictor on the log odds of the outcome.\n",
    "      - **P-values**: Determine the significance of each predictor (significant if \\( p < 0.05 \\)).\n",
    "\n",
    "### 4. Visualize with Plotly\n",
    "   - Simulate \"best fit lines\" by plotting predicted probabilities for different values of the continuous variable (e.g., age) at each level of the binary variable (e.g., gender).\n",
    "   - Plot separate lines for each model and examine if the interaction term creates a significant difference in prediction patterns.\n",
    "\n",
    "### 5. Evaluate the Interaction Term\n",
    "   - Compare the visualizations of both models:\n",
    "      - If the lines diverge noticeably in the interaction model, the interaction term likely adds value.\n",
    "      - If they are similar, the interaction term might not be necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67457d",
   "metadata": {},
   "source": [
    "### 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the *coefficients* are larger than 10 while having *strong* or *very strong evidence against* the *null hypothesis* of 'no effect'\"<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af2748",
   "metadata": {},
   "source": [
    "The apparent contradiction between \"the model only explains 17.6% of the variability in the data\" (a low \\( R^2 \\) value) and \"many coefficients are larger than 10 with strong or very strong evidence against the null hypothesis of 'no effect'\" (based on p-values) arises because **R-squared and p-values measure different aspects of model performance** and answer distinct questions about the model.\n",
    "\n",
    "### Understanding R-Squared and P-Values in Context\n",
    "1. **R-Squared (\\( R^2 \\))**: This metric describes the proportion of variation in the outcome variable (e.g., \"HP\" in this example) that the model can explain. An \\( R^2 \\) of 17.6% indicates that the model only accounts for a small portion of the variation in the data. This can happen if:\n",
    "   - There is a lot of **unexplained variability** in the outcome (HP) due to other factors not included in the model.\n",
    "   - The model fits the data moderately well but still leaves substantial variability that is random or due to unmeasured factors.\n",
    "\n",
    "2. **P-Values and Coefficients**: Each predictor’s p-value evaluates whether there is evidence against the null hypothesis that the predictor has no effect on the outcome. Strong evidence (e.g., \\( p < 0.01 \\)) indicates that specific predictors, like \"Sp. Def\" or categorical levels within \"Generation,\" have statistically significant associations with the outcome. Large coefficients (over 10 in this case) suggest that these predictors have a meaningful relationship with the outcome. The p-values and coefficients tell us:\n",
    "   - **Significance**: The predictors likely influence the outcome variable when considered individually.\n",
    "   - **Effect Size**: Large coefficients imply that changes in these predictors could lead to substantial shifts in the outcome, assuming the predictor is meaningful and interpretable.\n",
    "\n",
    "### Why Low R-Squared and Significant Predictors Can Coexist\n",
    "- **Limited Explanatory Power**: Even with significant predictors, the model might not capture much of the overall variability if other important predictors are missing or if the outcome is inherently variable.\n",
    "- **Effect vs. Explanation**: A predictor can be highly significant (have a low p-value) and have a large effect size (a high coefficient), indicating a strong association with the outcome. However, if this predictor alone does not explain the outcome well compared to all other unknown factors, the overall \\( R^2 \\) will still be low.\n",
    "\n",
    "In other words, **R-squared measures how well the model explains overall variability**, while **p-values assess the likelihood that each predictor’s effect is due to chance**. These measures are not contradictory; they simply address different aspects of the model:\n",
    "- **R-squared** gives a sense of the overall model's explanatory power.\n",
    "- **P-values and coefficients** provide insights into the impact and reliability of individual predictors.\n",
    "\n",
    "### Practical Interpretation\n",
    "In practical terms:\n",
    "- **The model may have statistically significant predictors (small p-values)**, meaning certain variables do have an influence on the outcome.\n",
    "- **The low R-squared indicates that the model doesn’t capture much of the total variability**—suggesting that other factors, not included in the model, may also affect the outcome significantly.\n",
    "\n",
    "This outcome is common in real-world data, where complex, unmeasured factors often contribute to variability. It also highlights the importance of considering multiple metrics when evaluating a model’s performance rather than relying solely on R-squared or p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1d742",
   "metadata": {},
   "source": [
    "### 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e828979",
   "metadata": {},
   "source": [
    "This code examines **model generalizability** by comparing \"in-sample\" and \"out-of-sample\" performance metrics (measured by \\( R^2 \\)) for two models built on the Pokémon dataset. Here’s a breakdown of each part and the concepts it illustrates.\n",
    "\n",
    "### Code Overview\n",
    "1. **Data Preparation**: \n",
    "   - The Pokémon data is split into training (50%) and testing (50%) datasets. This split is necessary to evaluate \"in-sample\" and \"out-of-sample\" performance.\n",
    "   - \"None\" is used to replace missing values in the \"Type 2\" column to ensure the model can handle this data.\n",
    "\n",
    "2. **Model 3 (Simpler Model)**:\n",
    "   - **Specification**: Model 3 uses `HP` as the outcome and `Attack` and `Defense` as predictors.\n",
    "   - **In-Sample Performance**: After fitting the model to the training data, we calculate the **in-sample \\( R^2 \\)**, which reflects how well the model explains the variability in the training dataset.\n",
    "   - **Out-of-Sample Performance**: The fitted model is then used to predict `HP` in the testing dataset. By calculating the squared correlation between the predicted and actual `HP` values in the test set, we get the **out-of-sample \\( R^2 \\)**, which shows how well the model generalizes to new data.\n",
    "\n",
    "3. **Model 4 (Complex Model)**:\n",
    "   - **Specification**: Model 4 includes multiple interactions between `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk` to capture more complex relationships. This leads to a more intricate model compared to Model 3.\n",
    "   - **In-Sample Performance**: This model is also fitted on the training data, and its in-sample \\( R^2 \\) is calculated.\n",
    "   - **Out-of-Sample Performance**: Using the test data, the out-of-sample \\( R^2 \\) is calculated similarly to Model 3.\n",
    "\n",
    "### Explanation of Results and Concepts\n",
    "\n",
    "#### 1. **In-Sample vs. Out-of-Sample \\( R^2 \\)**\n",
    "   - **In-Sample \\( R^2 \\)**: Reflects how well each model fits the training data. Higher values indicate that the model explains a large portion of the variability in the training data.\n",
    "   - **Out-of-Sample \\( R^2 \\)**: Reflects the model’s ability to generalize to unseen data. This is essential for assessing whether the model is **overfitting** (capturing noise rather than the underlying relationships).\n",
    "\n",
    "#### 2. **Illustration of Overfitting**\n",
    "   - **Model 3** likely has a smaller in-sample \\( R^2 \\) than Model 4 because it’s a simpler model. Its out-of-sample \\( R^2 \\) should be relatively close to the in-sample \\( R^2 \\), showing that it generalizes reasonably well.\n",
    "   - **Model 4** has more predictors and interactions, which can lead to a higher in-sample \\( R^2 \\), suggesting it fits the training data very well. However, if the out-of-sample \\( R^2 \\) for Model 4 is much lower than its in-sample \\( R^2 \\), this would indicate overfitting: the model captures specific patterns in the training data that don’t apply to the test data.\n",
    "\n",
    "### Key Takeaways\n",
    "- **In-sample performance** tells us how well the model fits the data it was trained on, but it doesn’t guarantee generalizability.\n",
    "- **Out-of-sample performance** gives a more accurate assessment of the model’s ability to make predictions on new data.\n",
    "- When there’s a large gap between in-sample and out-of-sample \\( R^2 \\), it’s a sign that the model might be overfit, meaning it performs well on training data but poorly on unseen data. This balance between in-sample and out-of-sample metrics is essential for creating models that generalize effectively.\n",
    "\n",
    "This exercise emphasizes the importance of **testing model performance on both training and testing datasets** to ensure that a model is not only accurate on known data but also reliable for future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257dafc",
   "metadata": {},
   "source": [
    "\n",
    "### Summary of Key Steps and Concepts:\n",
    "1. **Data Split**: The Pokémon dataset is split into training (50%) and testing (50%) sets. This split is crucial for evaluating both \"in-sample\" (training data) and \"out-of-sample\" (testing data) model performance.\n",
    "\n",
    "2. **Model 3 (Simpler Model)**:\n",
    "   - Uses `Attack` and `Defense` as predictors for `HP`.\n",
    "   - Calculates both in-sample and out-of-sample \\( R^2 \\).\n",
    "   - **In-Sample \\( R^2 \\)**: Measures the model’s fit on the training data.\n",
    "   - **Out-of-Sample \\( R^2 \\)**: Measures the model’s predictive accuracy on the test data, helping assess its generalizability.\n",
    "\n",
    "3. **Model 4 (Complex Model)**:\n",
    "   - Adds multiple interaction terms among predictors like `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk`, making it more complex.\n",
    "   - Higher in-sample \\( R^2 \\) is expected due to its complexity.\n",
    "   - If the out-of-sample \\( R^2 \\) is much lower, it indicates **overfitting**—the model captures noise in the training data that doesn’t generalize to new data.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **In-sample \\( R^2 \\)** shows how well the model fits the training data.\n",
    "- **Out-of-sample \\( R^2 \\)** provides insight into the model’s generalizability.\n",
    "- Large discrepancies between in-sample and out-of-sample \\( R^2 \\) suggest overfitting, meaning the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "This approach highlights the importance of balancing model complexity with generalizability to ensure the model is both accurate on known data and reliable for future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18af2c",
   "metadata": {},
   "source": [
    "### 6. Work with a ChatBot to understand how the *model4_linear_form* (*linear form* specification of  *model4*) creates new *predictor variables* as the columns of the so-called \"design matrix\" *model4_spec.exog* (*model4_spec.exog.shape*) used to predict the *outcome variable*  *model4_spec.endog* and why the so-called *multicollinearity* in this \"design matrix\" (observed in *np.corrcoef(model4_spec.exog)*) contribues to the lack of \"out of sample\" *generalization* of *predictions* from *model4_fit*; then, explain this consisely in your own works<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b7a9f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Design Matrix and Predictor Creation**:\n",
    "   - The `model4_linear_form` defines a very complex model with multiple predictors and interaction terms among variables like `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk`.\n",
    "   - This model’s **design matrix** (`model4_spec.exog`) contains columns for each predictor and interaction term, creating a high-dimensional space with many predictor variables.\n",
    "   - The high number of predictors, especially interaction terms, results in a design matrix where many variables are highly correlated with each other.\n",
    "\n",
    "2. **Multicollinearity and the Condition Number**:\n",
    "   - **Multicollinearity** occurs when predictor variables are strongly correlated, making it difficult to distinguish their individual effects.\n",
    "   - The **condition number** of the design matrix (found in the model summary) is a diagnostic measure of multicollinearity. A high condition number indicates severe multicollinearity.\n",
    "   - Model 4’s condition number remains extremely high (e.g., in the trillions) even after **centering and scaling** the predictors. This high value signals a significant multicollinearity problem that undermines the stability of coefficient estimates.\n",
    "\n",
    "3. **Impacts on Generalizability**:\n",
    "   - High multicollinearity means the model is overly sensitive to small variations in the data, leading to **overfitting**.\n",
    "   - This results in a model that performs well on training data (high in-sample \\( R^2 \\)) but fails to generalize to new data (low out-of-sample \\( R^2 \\)).\n",
    "   - By modeling noise rather than true relationships, the model’s predictions do not extend to data beyond the training set, confirming that it is overfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000fc89",
   "metadata": {},
   "source": [
    "### Summary Explanation\n",
    "\n",
    "In summary, the overly complex specification of Model 4 leads to excessive multicollinearity in its design matrix, as shown by an extraordinarily high condition number. This multicollinearity causes overfitting, which means the model captures random noise rather than meaningful patterns, leading to poor out-of-sample performance and lack of generalizability. Centering and scaling help reduce some multicollinearity but are insufficient here due to the model's extreme complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63e6ea",
   "metadata": {},
   "source": [
    "### 7. Discuss with a ChatBot the rationale and principles by which *model5_linear_form* is  extended and developed from *model3_fit* and *model4_fit*; *model6_linear_form* is  extended and developed from *model5_linear_form*; and *model7_linear_form* is  extended and developed from *model6_linear_form*; then, explain this breifly and consisely in your own words<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9eed4a",
   "metadata": {},
   "source": [
    "The development of models from **Model 3** to **Model 7** illustrates a progression from simpler models with limited predictor variables to more complex models with targeted interactions and specific variables. This process focuses on balancing model complexity and predictive power while minimizing multicollinearity.\n",
    "\n",
    "### Rationale Behind Model Extensions\n",
    "\n",
    "1. **Model 5**:\n",
    "   - **Extension from Models 3 and 4**: Adds variables like `Speed`, `Sp. Def`, `Sp. Atk`, and categorical indicators for `Generation`, `Type 1`, and `Type 2`.\n",
    "   - **Rationale**: The added variables aim to improve prediction accuracy by including broader relevant features. The model is still kept simpler than Model 4 to avoid excessive multicollinearity and overfitting.\n",
    "   - **Result**: Model 5 performs better than earlier models without overfitting to the same degree as Model 4.\n",
    "\n",
    "2. **Model 6**:\n",
    "   - **Extension from Model 5**: Focuses on significant predictors identified in Model 5, including only the most influential continuous variables (`Attack`, `Speed`, `Sp. Def`, `Sp. Atk`) and selected categorical indicators (`Type 1` for \"Normal\" and \"Water\", and `Generation` indicators for 2 and 5).\n",
    "   - **Rationale**: By concentrating on significant predictors, Model 6 maintains predictive power while simplifying the model, reducing multicollinearity.\n",
    "   - **Result**: The model achieves strong in-sample and out-of-sample performance, suggesting better generalizability.\n",
    "\n",
    "3. **Model 7**:\n",
    "   - **Extension from Model 6**: Introduces targeted interactions among `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk` to capture potential synergistic effects while retaining the significant categorical indicators from Model 6.\n",
    "   - **Centering and Scaling**: Continuous predictors are centered and scaled to manage multicollinearity, resulting in a more reasonable condition number (15.4), which is low enough to indicate minimal multicollinearity issues.\n",
    "   - **Rationale**: This model seeks to capture complex relationships without introducing excessive multicollinearity.\n",
    "   - **Result**: Model 7 achieves improved predictive performance with manageable complexity, suggesting good generalizability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6eb49",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The progression from Models 3 to 7 involves adding meaningful predictors and interactions to improve predictive power while carefully controlling for multicollinearity. Centering and scaling continuous variables help manage multicollinearity in more complex models, allowing Model 7 to achieve better generalization than previous models without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55ce32",
   "metadata": {},
   "source": [
    "### 8. Work with a ChatBot to write a *for* loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" *model performance* metric actualizations (by not using *np.random.seed(130)* within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c832da",
   "metadata": {},
   "source": [
    "\n",
    "### Code Explanation\n",
    "The purpose of the code is to:\n",
    "1. Repeatedly split the dataset into training and testing sets (without fixing a random seed).\n",
    "2. Fit a model using the training data and compute its **in-sample \\( R^2 \\)**.\n",
    "3. Evaluate the model’s performance on the test data and compute the **out-of-sample \\( R^2 \\)**.\n",
    "4. Visualize how these metrics vary across different splits to understand the variability of model performance.\n",
    "\n",
    "Here’s the modified code for the Pokémon dataset using `model3_fit` as an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93154c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define model and repetitions\n",
    "linear_form = 'HP ~ Attack + Defense'\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "# Perform repeated train-test splits and model evaluations\n",
    "for i in range(reps):\n",
    "    # Randomly split data (50-50 split)\n",
    "    pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=0.5)\n",
    "    \n",
    "    # Fit model on training data\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=pokeaman_train).fit()\n",
    "    \n",
    "    # Calculate in-sample and out-of-sample R-squared\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    y_pred = final_model_fit.predict(pokeaman_test)\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(pokeaman_test.HP, y_pred)[0, 1] ** 2\n",
    "\n",
    "# Visualize results\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample R-squared\": in_sample_Rsquared,\n",
    "    \"Out of Sample R-squared\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "fig = px.scatter(df, x=\"In Sample R-squared\", y=\"Out of Sample R-squared\",\n",
    "                 title=\"In-Sample vs. Out-of-Sample R-squared across Random Splits\")\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", line_shape='linear'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e7edb",
   "metadata": {},
   "source": [
    "### Meaning of the Results\n",
    "1. **Variation Across Splits**:\n",
    "   - Both **in-sample** and **out-of-sample \\( R^2 \\)** will vary depending on how the dataset is split. Some splits might lead to more representative training and test sets, while others may result in overfitting or underfitting.\n",
    "\n",
    "2. **Overfitting**:\n",
    "   - If in-sample \\( R^2 \\) is consistently higher than out-of-sample \\( R^2 \\), the model may be overfitting—capturing noise rather than patterns.\n",
    "\n",
    "3. **Unusual Splits**:\n",
    "   - Occasionally, out-of-sample \\( R^2 \\) may exceed in-sample \\( R^2 \\), as seen in the initial example. This is due to random variation in splits where the test data might better align with the model’s predictions than the training data.\n",
    "\n",
    "4. **Purpose of Demonstration**:\n",
    "   - This process highlights the variability in model performance and demonstrates the importance of evaluating generalizability through multiple random splits rather than relying on a single split.\n",
    "   - It also provides insight into the stability of the model’s performance. If the spread of points is tight and close to the \\( y=x \\) line, the model is stable. A wider spread indicates variability in generalizability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fb9a5",
   "metadata": {},
   "source": [
    "\n",
    "### Summary of Findings\n",
    "This demonstration shows how random train-test splits affect model evaluation metrics and provides evidence of potential overfitting or instability. It emphasizes the importance of using repeated splits to assess the generalizability of models, ensuring they perform consistently across varied subsets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd99554",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This demonstration explores the variability of **in-sample** and **out-of-sample \\( R^2 \\)** metrics across multiple random train-test splits. The process involves repeatedly splitting the dataset, fitting a model on the training data, and calculating performance metrics on both training and testing datasets. Key insights include:\n",
    "\n",
    "1. **Variability in Metrics**: Both \\( R^2 \\) values vary depending on how the data is split, highlighting the role of randomness in model evaluation.\n",
    "\n",
    "2. **Overfitting Detection**:\n",
    "   - Consistently higher in-sample \\( R^2 \\) compared to out-of-sample \\( R^2 \\) suggests overfitting, where the model captures noise rather than generalizable patterns.\n",
    "   - Occasional cases where out-of-sample \\( R^2 \\) exceeds in-sample \\( R^2 \\) occur due to random alignment of test data with model predictions.\n",
    "\n",
    "3. **Purpose**:\n",
    "   - This approach evaluates a model’s generalizability by examining performance across many splits.\n",
    "   - It underscores the importance of avoiding reliance on a single train-test split and instead using multiple iterations to gauge model stability and robustness.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - A scatter plot of in-sample vs. out-of-sample \\( R^2 \\) helps identify trends and assess the consistency of the model’s performance. Points near the \\( y=x \\) line indicate better generalizability, while wider spreads suggest instability.\n",
    "\n",
    "Overall, this method ensures a thorough assessment of a model’s ability to generalize beyond the training dataset and provides insight into overfitting and performance variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb3866",
   "metadata": {},
   "source": [
    "### 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own words<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc63807",
   "metadata": {},
   "source": [
    "This exercise demonstrates the trade-off between model complexity and generalisation ability. By testing two models (model6_fit and model7_fit) on data partitioned in generational order, we find that complex models, while performing well under random partitioning, may reveal a lack of generalisation ability in real prediction scenarios.\n",
    "\n",
    "Model 7 is a more complex model containing multiple variables and higher-order interaction terms. Its ( R^2 ) performance is high when dividing the training and test data randomly, which seems to be better than Model 6. However, in the scenario of predicting by generation order, Model 7's performance drops dramatically, suggesting that it may capture noise in the training data that cannot be verified in the test data. This is an overfitting problem that complex models are prone to.\n",
    "\n",
    "In contrast, Model 6 is a simpler model that uses only a small number of variables and no higher-order interaction terms. Although its random division ( R^2 ) is slightly inferior to Model 7, it performs more consistently in sequential prediction. More importantly, Model 6 has smaller p-values for the coefficients, providing stronger statistical evidence of its more reliable predictive ability.\n",
    "\n",
    "In addition, the interpretability of the model is crucial; Model 7's complex interaction terms make it difficult to understand intuitively, while Model 6 is simpler and more interpretable. When the difference in predictive performance between two models is small, it is usually better to choose the simpler model because not only does it generalise better, it is also easier to understand by users and analysts.\n",
    "\n",
    "In summary, this exercise shows that it is important to test models not only by focusing on performance under random partitioning, but also by simulating real-world usage scenarios (e.g., time-based or sequential prediction). Simple models tend to perform more reliably and consistently in real prediction tasks.\n",
    "\n",
    "Translated with DeepL.com (free version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
